{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import transformers\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime as dt\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0,1,2,3'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model_name_dict = {\n",
    "    \"PubMedBERT\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "    \"biomed_roberta_base\": \"allenai/biomed_roberta_base\",\n",
    "    \"Bio_ClinicalBERT\":\"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "}\n",
    "\n",
    "class Hparams:\n",
    "    def __init__(self):\n",
    "        self.random_seed = 2021\n",
    "        self.data_dir = './data'\n",
    "        self.output_dir = './outputs'\n",
    "        self.batch_size = 256\n",
    "        self.token_max_length = 256\n",
    "        self.model_name = model_name_dict['PubMedBERT']\n",
    "        self.num_epochs = 5\n",
    "        self.class_1_weight = 130\n",
    "        self.initial_lr = 2e-5  # 2e-5\n",
    "        self.model_type = 'lstm_ex'  # cnn, lstm, lstm_ex\n",
    "        self.upsample_pos_n = 1\n",
    "        self.use_col = 'title_abstract'  # title, abstract, title_abstract\n",
    "        self.train_argument = True\n",
    "        self.cv_n = 5\n",
    "\n",
    "hps = Hparams()\n",
    "\n",
    "\n",
    "def seed_torch(seed:int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(hps.random_seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "orig_df = pd.read_csv(os.path.join(hps.data_dir, 'train.csv'), index_col=0)\n",
    "submit_df = pd.read_csv(os.path.join(hps.data_dir, 'test.csv'), index_col=0)\n",
    "sample_submit_df = pd.read_csv(os.path.join(hps.data_dir, 'sample_submit.csv'), index_col=0, header=None, names=['judgement'])\n",
    "\n",
    "# 修正\n",
    "orig_df.loc[2488, 'judgement'] = 0\n",
    "orig_df.loc[7708, 'judgement'] = 0\n",
    "\n",
    "# 補完\n",
    "orig_df['abstract'].fillna('', inplace=True)\n",
    "orig_df['title_abstract'] = orig_df.title + orig_df.abstract\n",
    "\n",
    "submit_df['abstract'].fillna('', inplace=True)\n",
    "submit_df['title_abstract'] = submit_df.title + submit_df.abstract\n",
    "submit_df['judgement'] = -1\n",
    "submit_df.reset_index(inplace=True, drop=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validations SetUp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_df, test_df = train_test_split(orig_df, test_size=0.2, random_state=hps.random_seed, shuffle=True, stratify=orig_df.judgement)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_cv_number(df, cv_n):\n",
    "\n",
    "    df['cv_id'] = 0\n",
    "\n",
    "    neg_idx = df.loc[df.judgement==0].index.tolist()\n",
    "    pos_idx = df.loc[df.judgement==1].index.tolist()\n",
    "\n",
    "    neg_idx = [list(a) for a in list(np.array_split(random.sample(neg_idx, len(neg_idx)), cv_n))]\n",
    "    pos_idx = [list(a) for a in list(np.array_split(random.sample(pos_idx, len(pos_idx)), cv_n))]\n",
    "\n",
    "    for i in range(cv_n):\n",
    "        n_id = neg_idx[i]\n",
    "        p_id = pos_idx[i]\n",
    "        df.loc[n_id, 'cv_id'] = i\n",
    "        df.loc[p_id, 'cv_id'] = i\n",
    "\n",
    "    df = df.sort_index()\n",
    "\n",
    "    for i in range(cv_n):\n",
    "        tmp_df = df.loc[df.cv_id==i]\n",
    "        print('cv_id:', i, '->  pos:', len(tmp_df.loc[tmp_df.judgement==1]), ' / neg:', len(tmp_df.loc[tmp_df.judgement==0]), ' / all:', len(tmp_df))\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_cv_number(train_df, cv_n=hps.cv_n)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cv_id: 0 ->  pos: 101  / neg: 4243  / all: 4344\n",
      "cv_id: 1 ->  pos: 101  / neg: 4243  / all: 4344\n",
      "cv_id: 2 ->  pos: 101  / neg: 4242  / all: 4343\n",
      "cv_id: 3 ->  pos: 101  / neg: 4242  / all: 4343\n",
      "cv_id: 4 ->  pos: 100  / neg: 4242  / all: 4342\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_69983/4072971457.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cv_id'] = 0\n",
      "/opt/miniconda3/envs/srws/lib/python3.9/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataSet / DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, use_col='title_abstract', token_max_length=512, argument=False, upsample_pos_n=1):\n",
    "\n",
    "        if upsample_pos_n > 1:\n",
    "            df_pos = df.loc[df.judgement==1]\n",
    "            df_pos = pd.concat([df_pos for i in range(int(upsample_pos_n))], axis=0).reset_index(drop=True)\n",
    "            df_neg = df.loc[df.judgement==0]\n",
    "            self.df = pd.concat([df_pos, df_neg], axis=0).reset_index(drop=True)\n",
    "        else:\n",
    "            self.df = df\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.argument = argument\n",
    "        self.use_col = use_col\n",
    "\n",
    "    def text_argument(self, text, drop_min_seq=3, seq_sort=True):\n",
    "        seq_list = text.split('. ')\n",
    "        seq_len = len(seq_list)\n",
    "        if seq_len >= drop_min_seq:\n",
    "            orig_idx_list = list(range(0, seq_len))\n",
    "            idx_list = random.sample(orig_idx_list, random.randint(round(seq_len * 0.7), seq_len))\n",
    "            if seq_sort:\n",
    "                idx_list = sorted(idx_list)\n",
    "            insert_idx_list = random.sample(orig_idx_list, random.randint(0, seq_len//3))\n",
    "            for x in insert_idx_list:\n",
    "                idx = random.randint(0, len(idx_list))\n",
    "                idx_list.insert(idx, x)\n",
    "            seq_list = [seq_list[i] for i in idx_list]\n",
    "        text = '. '.join(seq_list)\n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text = self.df.loc[idx, self.use_col]\n",
    "\n",
    "        if self.argument:\n",
    "            text = self.text_argument(text, drop_min_seq=3, seq_sort=True)\n",
    "\n",
    "        token = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            padding = 'max_length', max_length = hps.token_max_length, truncation = True,\n",
    "            return_attention_mask=True, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        sample = dict(\n",
    "            input_ids=token['input_ids'][0],\n",
    "            attention_mask=token['attention_mask'][0]\n",
    "        )\n",
    "        \n",
    "        label = torch.tensor(self.df.loc[idx, 'judgement'], dtype=torch.float32)\n",
    "        return sample, label\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datasets = {phase:TextClassificationDataset(df=phase_param['df'][phase], tokenizer=base_tokenizer, use_col=hps.use_col,\\\n",
    "                                            token_max_length=hps.token_max_length, argument=phase_param['argument'][phase],\\\n",
    "                                            upsample_pos_n=phase_param['upsample_pos_n'][phase]) for phase in ['train', 'val', 'test', 'submit']}\n",
    "\n",
    "dataloaders = {phase: DataLoader(datasets[phase], batch_size=phase_param['batch_size'][phase], \\\n",
    "                                 shuffle=phase_param['shuffle'][phase]) for phase in ['train', 'val', 'test', 'submit']}\n",
    "\n",
    "print(len(datasets['train']), len(datasets['val']), len(datasets['test']), len(datasets['submit']))\n",
    "print(len(dataloaders['train']), len(dataloaders['val']), len(dataloaders['test']), len(dataloaders['submit']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HuggingFace Transformers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "base_tokenizer = transformers.AutoTokenizer.from_pretrained(hps.model_name)\n",
    "\n",
    "bert_config = transformers.AutoConfig.from_pretrained(hps.model_name)\n",
    "bert_config.output_hidden_states = True\n",
    "\n",
    "bert = transformers.AutoModel.from_pretrained(hps.model_name, config=bert_config)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('srws': conda)"
  },
  "interpreter": {
   "hash": "7f2d3241de56b0fa9ccb32ec1dbd92e097a59df5b8abdff3a1f1414152af23a6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}