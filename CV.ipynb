{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import transformers\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime as dt\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0,1,2,3'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model_name_dict = {\n",
    "    \"PubMedBERT\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "    \"biomed_roberta_base\": \"allenai/biomed_roberta_base\",\n",
    "    \"Bio_ClinicalBERT\":\"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "}\n",
    "\n",
    "class Hparams:\n",
    "    def __init__(self):\n",
    "        self.random_seed = 2021\n",
    "        self.data_dir = './data'\n",
    "        self.output_dir = './outputs'\n",
    "        self.batch_size = 256\n",
    "        self.token_max_length = 256\n",
    "        self.model_name = model_name_dict['PubMedBERT']\n",
    "        self.num_epochs = 1\n",
    "        self.class_1_weight = 130\n",
    "        self.initial_lr = 2e-5  # 2e-5\n",
    "        self.model_type = 'lstm_ex'  # cnn, lstm, lstm_ex\n",
    "        self.upsample_pos_n = 1\n",
    "        self.use_col = 'title_abstract'  # title, abstract, title_abstract\n",
    "        self.train_argument = True\n",
    "        self.cv_n = 2\n",
    "\n",
    "hps = Hparams()\n",
    "\n",
    "\n",
    "def seed_torch(seed:int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(hps.random_seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "orig_df = pd.read_csv(os.path.join(hps.data_dir, 'train.csv'), index_col=0)\n",
    "submit_df = pd.read_csv(os.path.join(hps.data_dir, 'test.csv'), index_col=0)\n",
    "sample_submit_df = pd.read_csv(os.path.join(hps.data_dir, 'sample_submit.csv'), index_col=0, header=None, names=['judgement'])\n",
    "\n",
    "# 修正\n",
    "orig_df.loc[2488, 'judgement'] = 0\n",
    "orig_df.loc[7708, 'judgement'] = 0\n",
    "\n",
    "# 補完\n",
    "orig_df['abstract'].fillna('', inplace=True)\n",
    "orig_df['title_abstract'] = orig_df.title + orig_df.abstract\n",
    "\n",
    "submit_df['abstract'].fillna('', inplace=True)\n",
    "submit_df['title_abstract'] = submit_df.title + submit_df.abstract\n",
    "submit_df['judgement'] = -1\n",
    "submit_df.reset_index(inplace=True, drop=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validations SetUp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_df, test_df = train_test_split(orig_df, test_size=0.2, random_state=hps.random_seed, shuffle=True, stratify=orig_df.judgement)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_cv_number(df, cv_n):\n",
    "\n",
    "    df['cv_id'] = 0\n",
    "\n",
    "    neg_idx = df.loc[df.judgement==0].index.tolist()\n",
    "    pos_idx = df.loc[df.judgement==1].index.tolist()\n",
    "\n",
    "    neg_idx = [list(a) for a in list(np.array_split(random.sample(neg_idx, len(neg_idx)), cv_n))]\n",
    "    pos_idx = [list(a) for a in list(np.array_split(random.sample(pos_idx, len(pos_idx)), cv_n))]\n",
    "\n",
    "    for i in range(cv_n):\n",
    "        n_id = neg_idx[i]\n",
    "        p_id = pos_idx[i]\n",
    "        df.loc[n_id, 'cv_id'] = i\n",
    "        df.loc[p_id, 'cv_id'] = i\n",
    "\n",
    "    df = df.sort_index()\n",
    "\n",
    "    for i in range(cv_n):\n",
    "        tmp_df = df.loc[df.cv_id==i]\n",
    "        print('cv_id:', i, '->  pos:', len(tmp_df.loc[tmp_df.judgement==1]), ' / neg:', len(tmp_df.loc[tmp_df.judgement==0]), ' / all:', len(tmp_df))\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_cv_number(train_df, cv_n=hps.cv_n)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cv_id: 0 ->  pos: 252  / neg: 10606  / all: 10858\n",
      "cv_id: 1 ->  pos: 252  / neg: 10606  / all: 10858\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_95433/4072971457.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cv_id'] = 0\n",
      "/opt/miniconda3/envs/srws/lib/python3.9/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hugging Face"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "base_tokenizer = transformers.AutoTokenizer.from_pretrained(hps.model_name)\n",
    "\n",
    "bert_config = transformers.AutoConfig.from_pretrained(hps.model_name)\n",
    "bert_config.output_hidden_states = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataSet / DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, use_col='title_abstract', token_max_length=512, argument=False, upsample_pos_n=1):\n",
    "\n",
    "        if upsample_pos_n > 1:\n",
    "            df_pos = df.loc[df.judgement==1]\n",
    "            df_pos = pd.concat([df_pos for i in range(int(upsample_pos_n))], axis=0).reset_index(drop=True)\n",
    "            df_neg = df.loc[df.judgement==0]\n",
    "            self.df = pd.concat([df_pos, df_neg], axis=0).reset_index(drop=True)\n",
    "        else:\n",
    "            self.df = df\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.argument = argument\n",
    "        self.use_col = use_col\n",
    "\n",
    "    def text_argument(self, text, drop_min_seq=3, seq_sort=True):\n",
    "        seq_list = text.split('. ')\n",
    "        seq_len = len(seq_list)\n",
    "        if seq_len >= drop_min_seq:\n",
    "            orig_idx_list = list(range(0, seq_len))\n",
    "            idx_list = random.sample(orig_idx_list, random.randint(round(seq_len * 0.7), seq_len))\n",
    "            if seq_sort:\n",
    "                idx_list = sorted(idx_list)\n",
    "            insert_idx_list = random.sample(orig_idx_list, random.randint(0, seq_len//3))\n",
    "            for x in insert_idx_list:\n",
    "                idx = random.randint(0, len(idx_list))\n",
    "                idx_list.insert(idx, x)\n",
    "            seq_list = [seq_list[i] for i in idx_list]\n",
    "        text = '. '.join(seq_list)\n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text = self.df.loc[idx, self.use_col]\n",
    "\n",
    "        if self.argument:\n",
    "            text = self.text_argument(text, drop_min_seq=3, seq_sort=True)\n",
    "\n",
    "        token = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            padding = 'max_length', max_length = hps.token_max_length, truncation = True,\n",
    "            return_attention_mask=True, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        sample = dict(\n",
    "            input_ids=token['input_ids'][0],\n",
    "            attention_mask=token['attention_mask'][0]\n",
    "        )\n",
    "        \n",
    "        label = torch.tensor(self.df.loc[idx, 'judgement'], dtype=torch.float32)\n",
    "        return sample, label\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class BertLstmExModel(nn.Module):\n",
    "    def __init__(self, hidden_size, config, use_hidden_n=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = transformers.AutoModel.from_pretrained(hps.model_name, config=bert_config)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_hidden_n = use_hidden_n\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.conv1d = nn.Conv1d(in_channels=self.use_hidden_n, out_channels=1, kernel_size=3, padding='same')\n",
    "        self.regressor = nn.Linear(self.hidden_size*2, 1)\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states_list = [outputs['hidden_states'][-1*i] for i in range(1, self.use_hidden_n+1)]\n",
    "        self.lstm.flatten_parameters()\n",
    "        out_list = [\n",
    "            self.dropout(\n",
    "                self.leakyrelu(\n",
    "                    self.lstm(hidden_state, None)[0]\n",
    "                )[:, -1, :]\n",
    "            ).view(-1, 1, self.hidden_size*2)  # (batch, use_hidden_n, hidden_size*2)\n",
    "        for hidden_state in hidden_states_list]\n",
    "\n",
    "        out = torch.cat(out_list, dim=1)\n",
    "\n",
    "        out = self.dropout(self.leakyrelu(self.conv1d(out)))\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        logits = torch.flatten(self.regressor(out))\n",
    "        return logits\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checkpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class ModelCheckpoint:\n",
    "    def __init__(self, save_dir:str, save_name:str, cv_id:int):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.cv_id = cv_id\n",
    "        self.save_dir = save_dir\n",
    "        self.save_name = save_name\n",
    "        self.best_loss = self.best_acc = self.best_fbeta_score = 0.0\n",
    "\n",
    "    def get_checkpoint_name(self):\n",
    "        checkpoint_name = f\"{self.save_name.replace('/', '_')}_cv{self.cv_id}.pth\"\n",
    "        checkpoint_name = os.path.join(self.save_dir, checkpoint_name)\n",
    "        return checkpoint_name\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.get_checkpoint_name(self.cv_id))\n",
    "\n",
    "    def load_checkpoint(self, model=None, manual_name=None):\n",
    "        if manual_name is None:\n",
    "            checkpoint_name = self.get_checkpoint_name(self.cv_id)\n",
    "        else:\n",
    "            checkpoint_name = manual_name\n",
    "        print(checkpoint_name)\n",
    "        model.load_state_dict(torch.load(checkpoint_name))\n",
    "        return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def fit(dataloaders, model, optimizer, num_epochs, device, batch_size, lr_scheduler, cv_id):\n",
    "\n",
    "    history = {\n",
    "        'train':{'loss':[], 'acc':[], 'fbscore':[]},\n",
    "        'val':{'loss':[], 'acc':[], 'fbscore':[]},\n",
    "        'lr':[],\n",
    "    }\n",
    "\n",
    "    checkpoint = ModelCheckpoint(save_dir='model_weights', save_name='bert_text_classification', cv_id=cv_id)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"Using device : {device}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"【Epoch {epoch+1: 3}/{num_epochs: 3}】   LR -> \", end='')\n",
    "        for i, params in enumerate(optimizer.param_groups):\n",
    "            print(f\"Group{i}: {params['lr']:.4e}\", end=' / ')\n",
    "        print('')\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_fbeta_score = 0.0\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            for i, (inputs, labels) in enumerate(tqdm(dataloaders[phase])):\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    logits_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    pos_weight = torch.tensor([hps.class_1_weight for i in range(input_ids.size(0))]).to(device)\n",
    "                    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "                    loss = criterion(logits_outputs, labels)\n",
    "\n",
    "                    outputs = torch.sigmoid(logits_outputs)\n",
    "                    preds = torch.where(outputs >= 0.5, 1, 0)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        lr_scheduler.step()\n",
    "\n",
    "                running_loss += loss.item() * input_ids.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                running_fbeta_score += fbeta_score(labels.to('cpu').detach().numpy(), preds.to('cpu').detach().numpy(), beta=7.0, zero_division=0) * input_ids.size(0)    \n",
    "\n",
    "                if phase == 'train':\n",
    "                    if i % 10 == 9:\n",
    "                        total_num = float((i * batch_size) + input_ids.size(0))\n",
    "                        print(f\"{i+1: 4}/{len(dataloaders[phase]): 4}  <{phase}> Loss:{(running_loss/total_num):.4f}  Acc:{(running_corrects/total_num):.4f}  fbScore:{(running_fbeta_score/total_num):.4f}   LR -> \", end='')\n",
    "                        for i, params in enumerate(optimizer.param_groups):\n",
    "                            print(f\"Group{i}: {params['lr']:.4e}\", end=' / ')\n",
    "                            if isinstance(optimizer.param_groups[0]['lr'], float):\n",
    "                                history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "                            else:\n",
    "                                history['lr'].append(optimizer.param_groups[0]['lr'].item())\n",
    "                        print('')\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            epoch_fbscore = running_fbeta_score / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(f\"<{phase}> Loss:{epoch_loss:.4f}  Acc:{epoch_acc:.4f}  fbScore:{epoch_fbscore:.4f}\")\n",
    "\n",
    "            history[phase]['loss'].append(epoch_loss)\n",
    "            history[phase]['acc'].append(epoch_acc.item())\n",
    "            history[phase]['fbscore'].append(epoch_fbscore)\n",
    "\n",
    "\n",
    "            if phase == 'val' and epoch_fbscore > checkpoint.best_fbeta_score:\n",
    "                print(f\"Checkpoints have been updated to the epoch {epoch+1} weights.\")\n",
    "                checkpoint.best_loss = epoch_loss\n",
    "                checkpoint.best_acc = epoch_acc\n",
    "                checkpoint.best_fbeta_score = epoch_fbscore\n",
    "                checkpoint.best_epoch = epoch+1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print('-' * 150)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    checkpoint.save_checkpoint(model)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def inference(model, dataloader, device, evaluate=True):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_fbeta_score = 0.0\n",
    "\n",
    "    preds_labels_dict = dict(preds = np.empty(0), labels = np.empty(0))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            if evaluate:\n",
    "                pos_weight = torch.tensor([hps.class_1_weight for i in range(input_ids.size(0))]).to(device)\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "                loss = criterion(logits_outputs, labels)\n",
    "\n",
    "            outputs = torch.sigmoid(logits_outputs)\n",
    "            preds = torch.where(outputs >= 0.5, 1, 0)\n",
    "            \n",
    "            if evaluate:\n",
    "                running_loss += loss.item() * input_ids.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                running_fbeta_score += fbeta_score(labels.to('cpu').detach().numpy(), preds.to('cpu').detach().numpy(), beta=7.0, zero_division=0) * input_ids.size(0)\n",
    "\n",
    "            preds_labels_dict['preds']  = np.hstack([preds_labels_dict['preds'], preds.to('cpu').detach().numpy().copy()])\n",
    "            preds_labels_dict['labels']  = np.hstack([preds_labels_dict['labels'], labels.to('cpu').detach().numpy().copy()])\n",
    "    \n",
    "    if evaluate:\n",
    "        loss = running_loss / len(dataloader.dataset)\n",
    "        acc = running_corrects / len(dataloader.dataset)\n",
    "        fbscore = running_fbeta_score / len(dataloader.dataset)\n",
    "        print(f\"Loss:{loss:.4f}  Acc:{acc:.4f}  fbScore:{fbscore:.4f}\")\n",
    "    return preds_labels_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CrossValidation Loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def model_setup(model, dataloaders):\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        params=[\n",
    "            {'params': model.bert.embeddings.parameters(), 'lr': 1e-5},\n",
    "            {'params': model.bert.encoder.parameters(), 'lr': 3e-5},\n",
    "            {'params': model.bert.pooler.parameters(), 'lr': 5e-5},\n",
    "            {'params': model.lstm.parameters(), 'lr': 5e-4},\n",
    "            {'params': model.conv1d.parameters(), 'lr': 5e-4},\n",
    "            {'params': model.regressor.parameters(), 'lr': 1e-3}\n",
    "        ]\n",
    "    )\n",
    "    num_warmup_steps = round(hps.num_epochs * len(dataloaders['train']) * 0.1)\n",
    "    num_training_steps = round(hps.num_epochs * len(dataloaders['train']))\n",
    "    print(f\"InitLR:{hps.initial_lr} / num_warmup_steps:{num_warmup_steps} / num_training_steps:{num_training_steps}\")\n",
    "    lr_scheduler = transformers.get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=num_warmup_steps, \n",
    "                                                                num_training_steps=num_training_steps, last_epoch=-1)\n",
    "\n",
    "    return (optimizer, lr_scheduler)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def cross_validation(cv_n, orig_df):\n",
    "\n",
    "    for i in range(cv_n):\n",
    "        print('\\033[32m' + f\"Cross-validation loop : {i+1}/{cv_n}\" + '\\033[0m')\n",
    "\n",
    "        # DataFrame\n",
    "        train_df = orig_df.loc[orig_df.cv_id != i].copy().reset_index(drop=True)\n",
    "        valid_df = orig_df.loc[orig_df.cv_id == i].copy().reset_index(drop=True)\n",
    "        print(f\"Train  ->  label_1:{train_df.judgement.sum()} / all:{train_df.judgement.count()}   ({train_df.judgement.sum() / train_df.judgement.count() * 100:.3f}%)\")\n",
    "        print(f\"Valid  ->  label_1:{valid_df.judgement.sum()} / all:{valid_df.judgement.count()}   ({valid_df.judgement.sum() / valid_df.judgement.count() * 100:.3f}%)\")\n",
    "\n",
    "        # Dataset / Dataloader\n",
    "        phase_param = {\n",
    "            \"df\":{'train': train_df, 'val': valid_df}, \"argument\":{'train': hps.train_argument, 'val': False}, \"batch_size\":{'train':hps.batch_size, \n",
    "            'val':hps.batch_size*4}, \"shuffle\":{'train': True, 'val': False}, \"upsample_pos_n\":{'train': hps.upsample_pos_n, 'val': 1}}\n",
    "        datasets = {phase:TextClassificationDataset(df=phase_param['df'][phase], tokenizer=base_tokenizer, use_col=hps.use_col,\\\n",
    "                                            token_max_length=hps.token_max_length, argument=phase_param['argument'][phase],\\\n",
    "                                            upsample_pos_n=phase_param['upsample_pos_n'][phase]) for phase in ['train', 'val']}\n",
    "        dataloaders = {phase: DataLoader(datasets[phase], batch_size=phase_param['batch_size'][phase], \\\n",
    "                                        shuffle=phase_param['shuffle'][phase]) for phase in ['train', 'val']}\n",
    "        \n",
    "        # Model / Optimizer\n",
    "        model = BertLstmExModel(hidden_size=bert_config.hidden_size, config=bert_config, use_hidden_n=10)\n",
    "        optimizer, lr_scheduler = model_setup(model, dataloaders)\n",
    "        model = model.to(device)\n",
    "        device_num = torch.cuda.device_count()\n",
    "        if device_num > 1:\n",
    "            print(f\"Use {device_num} GPUs\")\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        # Training / Validation\n",
    "        model, history = fit(dataloaders=dataloaders, model=model, optimizer=optimizer, num_epochs=hps.num_epochs, \n",
    "                             device=device, batch_size=hps.batch_size, lr_scheduler=lr_scheduler, cv_id=i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        del model, datasets, dataloaders\n",
    "        torch.cuda.empty_cache()\n",
    "        print()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "cross_validation(cv_n=hps.cv_n, orig_df=train_df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[32mCross-validation loop : 1/2\u001b[0m\n",
      "Train  ->  label_1:252 / all:10858   (2.321%)\n",
      "Valid  ->  label_1:252 / all:10858   (2.321%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "InitLR:2e-05 / num_warmup_steps:4 / num_training_steps:43\n",
      "Use 4 GPUs\n",
      "Using device : cuda\n",
      "【Epoch   1/  1】   LR -> Group0: 0.0000e+00 / Group1: 0.0000e+00 / Group2: 0.0000e+00 / Group3: 0.0000e+00 / Group4: 0.0000e+00 / Group5: 0.0000e+00 / \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d359c79ef2574650b8a052db51d52699"
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  10/  43  <train> Loss:2.5890  Acc:0.1414  fbScore:0.5034   LR -> Group0: 8.4615e-06 / Group1: 2.5385e-05 / Group2: 4.2308e-05 / Group3: 4.2308e-04 / Group4: 4.2308e-04 / Group5: 8.4615e-04 / \n",
      "  20/  43  <train> Loss:2.3912  Acc:0.0814  fbScore:0.4996   LR -> Group0: 5.8974e-06 / Group1: 1.7692e-05 / Group2: 2.9487e-05 / Group3: 2.9487e-04 / Group4: 2.9487e-04 / Group5: 5.8974e-04 / \n",
      "  30/  43  <train> Loss:2.3155  Acc:0.0617  fbScore:0.5070   LR -> Group0: 3.3333e-06 / Group1: 1.0000e-05 / Group2: 1.6667e-05 / Group3: 1.6667e-04 / Group4: 1.6667e-04 / Group5: 3.3333e-04 / \n",
      "  40/  43  <train> Loss:2.2698  Acc:0.0521  fbScore:0.5108   LR -> Group0: 7.6923e-07 / Group1: 2.3077e-06 / Group2: 3.8462e-06 / Group3: 3.8462e-05 / Group4: 3.8462e-05 / Group5: 7.6923e-05 / \n",
      "<train> Loss:2.2756  Acc:0.0509  fbScore:0.5168\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7682f8aaed2d4cfcac46e40c6dc03e66"
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<val> Loss:2.0904  Acc:0.0232  fbScore:0.5404\n",
      "Checkpoints have been updated to the epoch 1 weights.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[32mCross-validation loop : 2/2\u001b[0m\n",
      "Train  ->  label_1:252 / all:10858   (2.321%)\n",
      "Valid  ->  label_1:252 / all:10858   (2.321%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "InitLR:2e-05 / num_warmup_steps:4 / num_training_steps:43\n",
      "Use 4 GPUs\n",
      "Using device : cuda\n",
      "【Epoch   1/  1】   LR -> Group0: 0.0000e+00 / Group1: 0.0000e+00 / Group2: 0.0000e+00 / Group3: 0.0000e+00 / Group4: 0.0000e+00 / Group5: 0.0000e+00 / \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f74ccf74b51d4be0a374f879e03879a0"
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  10/  43  <train> Loss:2.5588  Acc:0.2664  fbScore:0.4081   LR -> Group0: 8.4615e-06 / Group1: 2.5385e-05 / Group2: 4.2308e-05 / Group3: 4.2308e-04 / Group4: 4.2308e-04 / Group5: 8.4615e-04 / \n",
      "  20/  43  <train> Loss:2.3752  Acc:0.1443  fbScore:0.4571   LR -> Group0: 5.8974e-06 / Group1: 1.7692e-05 / Group2: 2.9487e-05 / Group3: 2.9487e-04 / Group4: 2.9487e-04 / Group5: 5.8974e-04 / \n",
      "  30/  43  <train> Loss:2.3136  Acc:0.1042  fbScore:0.4801   LR -> Group0: 3.3333e-06 / Group1: 1.0000e-05 / Group2: 1.6667e-05 / Group3: 1.6667e-04 / Group4: 1.6667e-04 / Group5: 3.3333e-04 / \n",
      "  40/  43  <train> Loss:2.2711  Acc:0.0838  fbScore:0.4911   LR -> Group0: 7.6923e-07 / Group1: 2.3077e-06 / Group2: 3.8462e-06 / Group3: 3.8462e-05 / Group4: 3.8462e-05 / Group5: 7.6923e-05 / \n",
      "<train> Loss:2.2685  Acc:0.0804  fbScore:0.4939\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edecbae8b9b64b8d9f824e7ebcd6db1f"
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_95433/3715180902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_95433/4077047596.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(cv_n, orig_df)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Training / Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         model, history = fit(dataloaders=dataloaders, model=model, optimizer=optimizer, num_epochs=hps.num_epochs, \n\u001b[0m\u001b[1;32m     33\u001b[0m                              device=device, batch_size=hps.batch_size, lr_scheduler=lr_scheduler)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_95433/866304856.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(dataloaders, model, optimizer, num_epochs, device, batch_size, lr_scheduler)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_95433/3133281308.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_min_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_sort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         token = self.tokenizer.encode_plus(\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_max_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2470\u001b[0m         )\n\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2473\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    479\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('srws': conda)"
  },
  "interpreter": {
   "hash": "7f2d3241de56b0fa9ccb32ec1dbd92e097a59df5b8abdff3a1f1414152af23a6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}