{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import transformers\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime as dt\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0,1,2,3'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model_name_dict = {\n",
    "    \"PubMedBERT\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "    \"biomed_roberta_base\": \"allenai/biomed_roberta_base\",\n",
    "    \"Bio_ClinicalBERT\":\"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "}\n",
    "\n",
    "class Hparams:\n",
    "    def __init__(self):\n",
    "        self.random_seed = 2021\n",
    "        self.data_dir = './data'\n",
    "        self.output_dir = './outputs'\n",
    "        self.batch_size = 256\n",
    "        self.token_max_length = 256\n",
    "        self.model_name = model_name_dict['PubMedBERT']\n",
    "        self.num_epochs = 1\n",
    "        self.class_1_weight = 130\n",
    "        self.initial_lr = 2e-5  # 2e-5\n",
    "        self.model_type = 'lstm_ex'  # cnn, lstm, lstm_ex\n",
    "        self.upsample_pos_n = 1\n",
    "        self.use_col = 'title_abstract'  # title, abstract, title_abstract\n",
    "        self.train_argument = True\n",
    "        self.cv_n = 2\n",
    "\n",
    "hps = Hparams()\n",
    "\n",
    "\n",
    "def seed_torch(seed:int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(hps.random_seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "orig_df = pd.read_csv(os.path.join(hps.data_dir, 'train.csv'), index_col=0)\n",
    "submit_df = pd.read_csv(os.path.join(hps.data_dir, 'test.csv'), index_col=0)\n",
    "sample_submit_df = pd.read_csv(os.path.join(hps.data_dir, 'sample_submit.csv'), index_col=0, header=None, names=['judgement'])\n",
    "\n",
    "# 修正\n",
    "orig_df.loc[2488, 'judgement'] = 0\n",
    "orig_df.loc[7708, 'judgement'] = 0\n",
    "\n",
    "# 補完\n",
    "orig_df['abstract'].fillna('', inplace=True)\n",
    "orig_df['title_abstract'] = orig_df.title + orig_df.abstract\n",
    "\n",
    "submit_df['abstract'].fillna('', inplace=True)\n",
    "submit_df['title_abstract'] = submit_df.title + submit_df.abstract\n",
    "submit_df['judgement'] = -1\n",
    "submit_df.reset_index(inplace=True, drop=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validations SetUp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_df, test_df = train_test_split(orig_df, test_size=0.2, random_state=hps.random_seed, shuffle=True, stratify=orig_df.judgement)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_cv_number(df, cv_n):\n",
    "\n",
    "    df['cv_id'] = 0\n",
    "\n",
    "    neg_idx = df.loc[df.judgement==0].index.tolist()\n",
    "    pos_idx = df.loc[df.judgement==1].index.tolist()\n",
    "\n",
    "    neg_idx = [list(a) for a in list(np.array_split(random.sample(neg_idx, len(neg_idx)), cv_n))]\n",
    "    pos_idx = [list(a) for a in list(np.array_split(random.sample(pos_idx, len(pos_idx)), cv_n))]\n",
    "\n",
    "    for i in range(cv_n):\n",
    "        n_id = neg_idx[i]\n",
    "        p_id = pos_idx[i]\n",
    "        df.loc[n_id, 'cv_id'] = i\n",
    "        df.loc[p_id, 'cv_id'] = i\n",
    "\n",
    "    df = df.sort_index()\n",
    "\n",
    "    for i in range(cv_n):\n",
    "        tmp_df = df.loc[df.cv_id==i]\n",
    "        print('cv_id:', i, '->  pos:', len(tmp_df.loc[tmp_df.judgement==1]), ' / neg:', len(tmp_df.loc[tmp_df.judgement==0]), ' / all:', len(tmp_df))\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_cv_number(train_df, cv_n=hps.cv_n)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cv_id: 0 ->  pos: 101  / neg: 4243  / all: 4344\n",
      "cv_id: 1 ->  pos: 101  / neg: 4243  / all: 4344\n",
      "cv_id: 2 ->  pos: 101  / neg: 4242  / all: 4343\n",
      "cv_id: 3 ->  pos: 101  / neg: 4242  / all: 4343\n",
      "cv_id: 4 ->  pos: 100  / neg: 4242  / all: 4342\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_87224/4072971457.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cv_id'] = 0\n",
      "/opt/miniconda3/envs/srws/lib/python3.9/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hugging Face"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "base_tokenizer = transformers.AutoTokenizer.from_pretrained(hps.model_name)\n",
    "\n",
    "bert_config = transformers.AutoConfig.from_pretrained(hps.model_name)\n",
    "bert_config.output_hidden_states = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataSet / DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, use_col='title_abstract', token_max_length=512, argument=False, upsample_pos_n=1):\n",
    "\n",
    "        if upsample_pos_n > 1:\n",
    "            df_pos = df.loc[df.judgement==1]\n",
    "            df_pos = pd.concat([df_pos for i in range(int(upsample_pos_n))], axis=0).reset_index(drop=True)\n",
    "            df_neg = df.loc[df.judgement==0]\n",
    "            self.df = pd.concat([df_pos, df_neg], axis=0).reset_index(drop=True)\n",
    "        else:\n",
    "            self.df = df\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.argument = argument\n",
    "        self.use_col = use_col\n",
    "\n",
    "    def text_argument(self, text, drop_min_seq=3, seq_sort=True):\n",
    "        seq_list = text.split('. ')\n",
    "        seq_len = len(seq_list)\n",
    "        if seq_len >= drop_min_seq:\n",
    "            orig_idx_list = list(range(0, seq_len))\n",
    "            idx_list = random.sample(orig_idx_list, random.randint(round(seq_len * 0.7), seq_len))\n",
    "            if seq_sort:\n",
    "                idx_list = sorted(idx_list)\n",
    "            insert_idx_list = random.sample(orig_idx_list, random.randint(0, seq_len//3))\n",
    "            for x in insert_idx_list:\n",
    "                idx = random.randint(0, len(idx_list))\n",
    "                idx_list.insert(idx, x)\n",
    "            seq_list = [seq_list[i] for i in idx_list]\n",
    "        text = '. '.join(seq_list)\n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text = self.df.loc[idx, self.use_col]\n",
    "\n",
    "        if self.argument:\n",
    "            text = self.text_argument(text, drop_min_seq=3, seq_sort=True)\n",
    "\n",
    "        token = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            padding = 'max_length', max_length = hps.token_max_length, truncation = True,\n",
    "            return_attention_mask=True, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        sample = dict(\n",
    "            input_ids=token['input_ids'][0],\n",
    "            attention_mask=token['attention_mask'][0]\n",
    "        )\n",
    "        \n",
    "        label = torch.tensor(self.df.loc[idx, 'judgement'], dtype=torch.float32)\n",
    "        return sample, label\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class BertLstmExModel(nn.Module):\n",
    "    def __init__(self, hidden_size, config, use_hidden_n=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = transformers.AutoModel.from_pretrained(hps.model_name, config=bert_config)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_hidden_n = use_hidden_n\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.conv1d = nn.Conv1d(in_channels=self.use_hidden_n, out_channels=1, kernel_size=3, padding='same')\n",
    "        self.regressor = nn.Linear(self.hidden_size*2, 1)\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states_list = [outputs['hidden_states'][-1*i] for i in range(1, self.use_hidden_n+1)]\n",
    "        self.lstm.flatten_parameters()\n",
    "        out_list = [\n",
    "            self.dropout(\n",
    "                self.leakyrelu(\n",
    "                    self.lstm(hidden_state, None)[0]\n",
    "                )[:, -1, :]\n",
    "            ).view(-1, 1, self.hidden_size*2)  # (batch, use_hidden_n, hidden_size*2)\n",
    "        for hidden_state in hidden_states_list]\n",
    "\n",
    "        out = torch.cat(out_list, dim=1)\n",
    "\n",
    "        out = self.dropout(self.leakyrelu(self.conv1d(out)))\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        logits = torch.flatten(self.regressor(out))\n",
    "        return logits\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checkpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class ModelCheckpoint:\n",
    "    def __init__(self, save_dir:str, model_name:str):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.save_dir = save_dir\n",
    "        self.model_name = model_name\n",
    "        jst = dt.timezone(dt.timedelta(hours=+9), 'JST')\n",
    "        dt_now = dt.datetime.now(jst)\n",
    "        self.dt_now_str = dt_now.strftime('%Y%m%d_%H%M')\n",
    "        self.best_loss = self.best_acc = self.best_fbeta_score = 0.0\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def get_checkpoint_name(self, epoch):\n",
    "        checkpoint_name = f\"{self.model_name.replace('/', '_')}__epoch{epoch:03}__{self.dt_now_str}.pth\"\n",
    "        checkpoint_name = os.path.join(self.save_dir, checkpoint_name)\n",
    "        return checkpoint_name\n",
    "\n",
    "    def save_checkpoint(self, model, epoch):\n",
    "        torch.save(model.state_dict(), self.get_checkpoint_name(epoch))\n",
    "\n",
    "    def load_checkpoint(self, model=None, epoch=1, manual_name=None):\n",
    "        if manual_name is None:\n",
    "            checkpoint_name = self.get_checkpoint_name(epoch)\n",
    "        else:\n",
    "            checkpoint_name = manual_name\n",
    "        print(checkpoint_name)\n",
    "        model.load_state_dict(torch.load(checkpoint_name))\n",
    "        return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def fit(dataloaders, model, optimizer, num_epochs, device, batch_size, lr_scheduler):\n",
    "\n",
    "    history = {\n",
    "        'train':{'loss':[], 'acc':[], 'fbscore':[]},\n",
    "        'val':{'loss':[], 'acc':[], 'fbscore':[]},\n",
    "        'lr':[],\n",
    "    }\n",
    "\n",
    "    checkpoint = ModelCheckpoint(save_dir='model_weights', model_name=hps.model_name)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"Using device : {device}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"【Epoch {epoch+1: 3}/{num_epochs: 3}】   LR -> \", end='')\n",
    "        for i, params in enumerate(optimizer.param_groups):\n",
    "            print(f\"Group{i}: {params['lr']:.4e}\", end=' / ')\n",
    "        print('')\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_fbeta_score = 0.0\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            for i, (inputs, labels) in enumerate(tqdm(dataloaders[phase])):\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    logits_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    pos_weight = torch.tensor([hps.class_1_weight for i in range(input_ids.size(0))]).to(device)\n",
    "                    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "                    loss = criterion(logits_outputs, labels)\n",
    "\n",
    "                    outputs = torch.sigmoid(logits_outputs)\n",
    "                    preds = torch.where(outputs >= 0.5, 1, 0)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        lr_scheduler.step()\n",
    "\n",
    "                running_loss += loss.item() * input_ids.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                running_fbeta_score += fbeta_score(labels.to('cpu').detach().numpy(), preds.to('cpu').detach().numpy(), beta=7.0, zero_division=0) * input_ids.size(0)    \n",
    "\n",
    "                if phase == 'train':\n",
    "                    if i % 10 == 9:\n",
    "                        total_num = float((i * batch_size) + input_ids.size(0))\n",
    "                        print(f\"{i+1: 4}/{len(dataloaders[phase]): 4}  <{phase}> Loss:{(running_loss/total_num):.4f}  Acc:{(running_corrects/total_num):.4f}  fbScore:{(running_fbeta_score/total_num):.4f}   LR -> \", end='')\n",
    "                        for i, params in enumerate(optimizer.param_groups):\n",
    "                            print(f\"Group{i}: {params['lr']:.4e}\", end=' / ')\n",
    "                            if isinstance(optimizer.param_groups[0]['lr'], float):\n",
    "                                history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "                            else:\n",
    "                                history['lr'].append(optimizer.param_groups[0]['lr'].item())\n",
    "                        print('')\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            epoch_fbscore = running_fbeta_score / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(f\"<{phase}> Loss:{epoch_loss:.4f}  Acc:{epoch_acc:.4f}  fbScore:{epoch_fbscore:.4f}\")\n",
    "\n",
    "            history[phase]['loss'].append(epoch_loss)\n",
    "            history[phase]['acc'].append(epoch_acc.item())\n",
    "            history[phase]['fbscore'].append(epoch_fbscore)\n",
    "\n",
    "\n",
    "            if phase == 'val' and epoch_fbscore > checkpoint.best_fbeta_score:\n",
    "                print(f\"Checkpoints have been updated to the epoch {epoch+1} weights.\")\n",
    "                checkpoint.best_loss = epoch_loss\n",
    "                checkpoint.best_acc = epoch_acc\n",
    "                checkpoint.best_fbeta_score = epoch_fbscore\n",
    "                checkpoint.best_epoch = epoch+1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print('-' * 150)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    checkpoint.save_checkpoint(model, epoch)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def inference(model, dataloader, device, evaluate=True):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_fbeta_score = 0.0\n",
    "\n",
    "    preds_labels_dict = dict(preds = np.empty(0), labels = np.empty(0))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            if evaluate:\n",
    "                pos_weight = torch.tensor([hps.class_1_weight for i in range(input_ids.size(0))]).to(device)\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "                loss = criterion(logits_outputs, labels)\n",
    "\n",
    "            outputs = torch.sigmoid(logits_outputs)\n",
    "            preds = torch.where(outputs >= 0.5, 1, 0)\n",
    "            \n",
    "            if evaluate:\n",
    "                running_loss += loss.item() * input_ids.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                running_fbeta_score += fbeta_score(labels.to('cpu').detach().numpy(), preds.to('cpu').detach().numpy(), beta=7.0, zero_division=0) * input_ids.size(0)\n",
    "\n",
    "            preds_labels_dict['preds']  = np.hstack([preds_labels_dict['preds'], preds.to('cpu').detach().numpy().copy()])\n",
    "            preds_labels_dict['labels']  = np.hstack([preds_labels_dict['labels'], labels.to('cpu').detach().numpy().copy()])\n",
    "    \n",
    "    if evaluate:\n",
    "        loss = running_loss / len(dataloader.dataset)\n",
    "        acc = running_corrects / len(dataloader.dataset)\n",
    "        fbscore = running_fbeta_score / len(dataloader.dataset)\n",
    "        print(f\"Loss:{loss:.4f}  Acc:{acc:.4f}  fbScore:{fbscore:.4f}\")\n",
    "    return preds_labels_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CrossValidation Loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def model_setup(model, dataloaders):\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        params=[\n",
    "            {'params': model.bert.embeddings.parameters(), 'lr': 1e-5},\n",
    "            {'params': model.bert.encoder.parameters(), 'lr': 3e-5},\n",
    "            {'params': model.bert.pooler.parameters(), 'lr': 5e-5},\n",
    "            {'params': model.lstm.parameters(), 'lr': 5e-4},\n",
    "            {'params': model.conv1d.parameters(), 'lr': 5e-4},\n",
    "            {'params': model.regressor.parameters(), 'lr': 1e-3}\n",
    "        ]\n",
    "    )\n",
    "    num_warmup_steps = round(hps.num_epochs * len(dataloaders['train']) * 0.1)\n",
    "    num_training_steps = round(hps.num_epochs * len(dataloaders['train']))\n",
    "    print(f\"InitLR:{hps.initial_lr} / num_warmup_steps:{num_warmup_steps} / num_training_steps:{num_training_steps}\")\n",
    "    lr_scheduler = transformers.get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=num_warmup_steps, \n",
    "                                                                num_training_steps=num_training_steps, last_epoch=-1)\n",
    "\n",
    "    return (optimizer, lr_scheduler)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def cross_validation(cv_n, orig_df):\n",
    "\n",
    "    for i in range(cv_n):\n",
    "        print('\\033[32m' + f\"Cross-validation loop : {i+1}/{cv_n}\" + '\\033[0m')\n",
    "\n",
    "        # DataFrame\n",
    "        train_df = orig_df.loc[orig_df.cv_id != i].copy().reset_index(drop=True)\n",
    "        valid_df = orig_df.loc[orig_df.cv_id == i].copy().reset_index(drop=True)\n",
    "        print(f\"Train  ->  label_1:{train_df.judgement.sum()} / all:{train_df.judgement.count()}   ({train_df.judgement.sum() / train_df.judgement.count() * 100:.3f}%)\")\n",
    "        print(f\"Valid  ->  label_1:{valid_df.judgement.sum()} / all:{valid_df.judgement.count()}   ({valid_df.judgement.sum() / valid_df.judgement.count() * 100:.3f}%)\")\n",
    "\n",
    "        # Dataset / Dataloader\n",
    "        phase_param = {\n",
    "            \"df\":{'train': train_df, 'val': valid_df}, \"argument\":{'train': hps.train_argument, 'val': False}, \"batch_size\":{'train':hps.batch_size, \n",
    "            'val':hps.batch_size*4}, \"shuffle\":{'train': True, 'val': False}, \"upsample_pos_n\":{'train': hps.upsample_pos_n, 'val': 1}}\n",
    "        datasets = {phase:TextClassificationDataset(df=phase_param['df'][phase], tokenizer=base_tokenizer, use_col=hps.use_col,\\\n",
    "                                            token_max_length=hps.token_max_length, argument=phase_param['argument'][phase],\\\n",
    "                                            upsample_pos_n=phase_param['upsample_pos_n'][phase]) for phase in ['train', 'val']}\n",
    "        dataloaders = {phase: DataLoader(datasets[phase], batch_size=phase_param['batch_size'][phase], \\\n",
    "                                        shuffle=phase_param['shuffle'][phase]) for phase in ['train', 'val']}\n",
    "        \n",
    "        # Model / Optimizer\n",
    "        model = BertLstmExModel(hidden_size=bert_config.hidden_size, config=bert_config, use_hidden_n=10)\n",
    "        optimizer, lr_scheduler = model_setup(model, dataloaders)\n",
    "        model = model.to(device)\n",
    "        device_num = torch.cuda.device_count()\n",
    "        if device_num > 1:\n",
    "            print(f\"Use {device_num} GPUs\")\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        # Training / Validation\n",
    "        model, history = fit(dataloaders=dataloaders, model=model, optimizer=optimizer, num_epochs=hps.num_epochs, \n",
    "                             device=device, batch_size=hps.batch_size, lr_scheduler=lr_scheduler)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        del model, datasets, dataloaders\n",
    "        torch.cuda.empty_cache()\n",
    "        print()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "cross_validation(cv_n=hps.cv_n, orig_df=train_df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[32mCross-validation loop : 1/5\u001b[0m\n",
      "Train  ->  label_1:403 / all:17372   (2.320%)\n",
      "Valid  ->  label_1:101 / all:4344   (2.325%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "InitLR:2e-05 / num_warmup_steps:34 / num_training_steps:340\n",
      "Use 4 GPUs\n",
      "Using device : cuda\n",
      "【Epoch   1/  5】   LR -> Group0: 0.0000e+00 / Group1: 0.0000e+00 / Group2: 0.0000e+00 / Group3: 0.0000e+00 / Group4: 0.0000e+00 / Group5: 0.0000e+00 / \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64f9015ba31445eba052740bb5675151"
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_87224/659122592.py\", line 17, in forward\n    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 990, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 582, in forward\n    layer_outputs = layer_module(\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 470, in forward\n    self_attention_outputs = self.attention(\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 401, in forward\n    self_outputs = self.self(\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 323, in forward\n    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\nRuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 31.75 GiB total capacity; 17.96 GiB already allocated; 128.75 MiB free; 18.16 GiB reserved in total by PyTorch)\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_87224/3715180902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_87224/4077047596.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(cv_n, orig_df)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Training / Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         model, history = fit(dataloaders=dataloaders, model=model, optimizer=optimizer, num_epochs=hps.num_epochs, \n\u001b[0m\u001b[1;32m     33\u001b[0m                              device=device, batch_size=hps.batch_size, lr_scheduler=lr_scheduler)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_87224/866304856.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(dataloaders, model, optimizer, num_epochs, device, batch_size, lr_scheduler)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0mlogits_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                     \u001b[0mpos_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_1_weight\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_87224/659122592.py\", line 17, in forward\n    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 990, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 582, in forward\n    layer_outputs = layer_module(\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 470, in forward\n    self_attention_outputs = self.attention(\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 401, in forward\n    self_outputs = self.self(\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/miniconda3/envs/srws/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 323, in forward\n    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\nRuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 31.75 GiB total capacity; 17.96 GiB already allocated; 128.75 MiB free; 18.16 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('srws': conda)"
  },
  "interpreter": {
   "hash": "7f2d3241de56b0fa9ccb32ec1dbd92e097a59df5b8abdff3a1f1414152af23a6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}