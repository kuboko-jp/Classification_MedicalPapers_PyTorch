{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import fbeta_score"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_141121/1532873424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfbeta_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 471\n",
    "seed_torch(seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Hparams:\n",
    "    def __init__(self):\n",
    "        self.data_dir = './data'\n",
    "        self.output_dir = './outputs'\n",
    "        self.batch_size = 64\n",
    "        self.token_max_length = 256\n",
    "        self.model_name = \"allenai/biomed_roberta_base\"\n",
    "        self.num_epochs = 10\n",
    "\n",
    "hps = Hparams()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_df = pd.read_csv(os.path.join(hps.data_dir, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(hps.data_dir, 'test.csv'))\n",
    "sample_submit_df = pd.read_csv(os.path.join(hps.data_dir, 'sample_submit.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>judgement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One-year age changes in MRI brain volumes in o...</td>\n",
       "      <td>Longitudinal studies indicate that declines in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Supportive CSF biomarker evidence to enhance t...</td>\n",
       "      <td>The present study was undertaken to validate t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Occurrence of basal ganglia germ cell tumors w...</td>\n",
       "      <td>Objective: To report a case series in which ba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New developments in diagnosis and therapy of C...</td>\n",
       "      <td>The etiology and pathogenesis of idiopathic ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27140</th>\n",
       "      <td>27140</td>\n",
       "      <td>The amyloidogenic pathway of amyloid precursor...</td>\n",
       "      <td>Amyloid beta-protein (A beta) is the main cons...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27141</th>\n",
       "      <td>27141</td>\n",
       "      <td>Technologic developments in radiotherapy and s...</td>\n",
       "      <td>We present a review of current technological p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27142</th>\n",
       "      <td>27142</td>\n",
       "      <td>Novel screening cascade identifies MKK4 as key...</td>\n",
       "      <td>Phosphorylation of Tau at serine 422 promotes ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27143</th>\n",
       "      <td>27143</td>\n",
       "      <td>Visualization of the gall bladder on F-18 FDOP...</td>\n",
       "      <td>The ability to label dihydroxyphenylalanine (D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27144</th>\n",
       "      <td>27144</td>\n",
       "      <td>Multidetector CT findings and differential dia...</td>\n",
       "      <td>OBJECTIVE: To compare the multidetector CT (MD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27145 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  One-year age changes in MRI brain volumes in o...   \n",
       "1          1  Supportive CSF biomarker evidence to enhance t...   \n",
       "2          2  Occurrence of basal ganglia germ cell tumors w...   \n",
       "3          3  New developments in diagnosis and therapy of C...   \n",
       "4          4  Prolonged shedding of SARS-CoV-2 in an elderly...   \n",
       "...      ...                                                ...   \n",
       "27140  27140  The amyloidogenic pathway of amyloid precursor...   \n",
       "27141  27141  Technologic developments in radiotherapy and s...   \n",
       "27142  27142  Novel screening cascade identifies MKK4 as key...   \n",
       "27143  27143  Visualization of the gall bladder on F-18 FDOP...   \n",
       "27144  27144  Multidetector CT findings and differential dia...   \n",
       "\n",
       "                                                abstract  judgement  \n",
       "0      Longitudinal studies indicate that declines in...          0  \n",
       "1      The present study was undertaken to validate t...          0  \n",
       "2      Objective: To report a case series in which ba...          0  \n",
       "3      The etiology and pathogenesis of idiopathic ch...          0  \n",
       "4                                                    NaN          0  \n",
       "...                                                  ...        ...  \n",
       "27140  Amyloid beta-protein (A beta) is the main cons...          0  \n",
       "27141  We present a review of current technological p...          0  \n",
       "27142  Phosphorylation of Tau at serine 422 promotes ...          0  \n",
       "27143  The ability to label dihydroxyphenylalanine (D...          0  \n",
       "27144  OBJECTIVE: To compare the multidetector CT (MD...          0  \n",
       "\n",
       "[27145 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_df.isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "id              0\n",
       "title           0\n",
       "abstract     4390\n",
       "judgement       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "base_tokenizer = transformers.AutoTokenizer.from_pretrained(hps.model_name)\n",
    "base_model = transformers.AutoModel.from_pretrained(hps.model_name)\n",
    "base_model_config = transformers.AutoConfig.from_pretrained(hps.model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(base_model_config)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, token_max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.title_tokenized = tokenizer.batch_encode_plus(\n",
    "            df.title.to_list(),\n",
    "            padding = 'max_length',            \n",
    "            max_length = token_max_length,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = dict(\n",
    "            input_ids=self.title_tokenized['input_ids'][idx],\n",
    "            attention_mask=self.title_tokenized['attention_mask'][idx]\n",
    "        )\n",
    "        label = torch.tensor(self.df.loc[idx, 'judgement'], dtype=torch.float32)\n",
    "        return title, label\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "train_ds = TextClassificationDataset(df=train_df, tokenizer=base_tokenizer, token_max_length=hps.token_max_length)\n",
    "token, label = train_ds[0]\n",
    "print(token['input_ids'].shape)\n",
    "print(token['input_ids'].shape)\n",
    "print(label)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=hps.batch_size, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "inputs, labels = next(iter(train_dl))\n",
    "print(inputs['input_ids'].shape)\n",
    "print(inputs['attention_mask'].shape)\n",
    "print(labels)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([64, 256])\n",
      "torch.Size([64, 256])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model, hidden_size):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.conv1d_1 = nn.Conv1d(hidden_size, 256, kernel_size=2, padding=1)\n",
    "        self.conv1d_2 = nn.Conv1d(256, 1, kernel_size=2, padding=1)\n",
    "        self.linear = nn.Linear(258, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = out['last_hidden_state'].permute(0, 2, 1)\n",
    "        conv_embed = torch.relu(self.conv1d_1(last_hidden_state))\n",
    "        conv_embed = self.conv1d_2(conv_embed).squeeze()\n",
    "        logits = torch.sigmoid(self.linear(conv_embed)).squeeze()\n",
    "        return logits\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model = TextClassificationModel(base_model=base_model, hidden_size=base_model_config.hidden_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "print(outputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.5214, 0.5158, 0.5237, 0.5164, 0.5151, 0.5174, 0.5205, 0.5088, 0.5186,\n",
      "        0.5170, 0.5185, 0.5193, 0.5228, 0.5140, 0.5170, 0.5196, 0.5151, 0.5209,\n",
      "        0.5234, 0.5234, 0.5175, 0.5157, 0.5204, 0.5148, 0.5156, 0.5195, 0.5183,\n",
      "        0.5148, 0.5212, 0.5225, 0.5203, 0.5200, 0.5228, 0.5182, 0.5192, 0.5098,\n",
      "        0.5225, 0.5221, 0.5142, 0.5235, 0.5176, 0.5195, 0.5173, 0.5140, 0.5185,\n",
      "        0.5194, 0.5202, 0.5160, 0.5149, 0.5230, 0.5205, 0.5180, 0.5174, 0.5176,\n",
      "        0.5177, 0.5170, 0.5191, 0.5214, 0.5176, 0.5174, 0.5193, 0.5057, 0.5179,\n",
      "        0.5179], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def train(train_loader, model, criterion, optimizer, num_epochs, ds_size, device, batch_size):\n",
    "    phase = 'train'\n",
    "    since = time.time()\n",
    "    print(f\"Using device : {device}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"【 Epoch {epoch+1: 3}/{num_epochs: 3} 】\")\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(tqdm(train_loader)):\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.where(outputs >= 0.5, 1, 0)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() + input_ids.size(0)\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "            if i % 10 == 9:\n",
    "                total_num = float((i * batch_size) + input_ids.size(0))\n",
    "                print(f\"{i+1: 4}/{len(train_loader): 4}  <{phase}> Loss:{(running_loss/total_num):.3f}  Acc:{(running_corrects/total_num):.3f}\")\n",
    "\n",
    "        epoch_loss = running_loss / ds_size\n",
    "        epoch_acc = running_loss / ds_size\n",
    "\n",
    "        print(f\"Epoch {epoch+1:3} Done.  <{phase}> Loss:{epoch_loss:.4f}  Acc:{epoch_acc:.4f}\")\n",
    "\n",
    "    \n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model = model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "model = train(train_loader=train_dl, model=model, criterion=criterion, \n",
    "              optimizer=optimizer, num_epochs=hps.num_epochs, ds_size=len(train_ds), device=device, batch_size=hps.batch_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device : cuda\n",
      "Epoch   1/ 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "923d480bc3f54e92a66aafda8b2238b1"
      },
      "text/plain": [
       "  0%|          | 0/425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10/425  <train> Loss:1.008  Acc:0.877\n",
      "20/425  <train> Loss:1.005  Acc:0.928\n",
      "30/425  <train> Loss:1.004  Acc:0.947\n",
      "40/425  <train> Loss:1.003  Acc:0.953\n",
      "50/425  <train> Loss:1.003  Acc:0.957\n",
      "60/425  <train> Loss:1.003  Acc:0.960\n",
      "70/425  <train> Loss:1.003  Acc:0.962\n",
      "80/425  <train> Loss:1.003  Acc:0.965\n",
      "90/425  <train> Loss:1.002  Acc:0.967\n",
      "100/425  <train> Loss:1.002  Acc:0.968\n",
      "110/425  <train> Loss:1.002  Acc:0.968\n",
      "120/425  <train> Loss:1.002  Acc:0.969\n",
      "130/425  <train> Loss:1.002  Acc:0.969\n",
      "140/425  <train> Loss:1.002  Acc:0.970\n",
      "150/425  <train> Loss:1.002  Acc:0.971\n",
      "160/425  <train> Loss:1.002  Acc:0.971\n",
      "170/425  <train> Loss:1.002  Acc:0.971\n",
      "180/425  <train> Loss:1.002  Acc:0.972\n",
      "190/425  <train> Loss:1.002  Acc:0.972\n",
      "200/425  <train> Loss:1.002  Acc:0.973\n",
      "210/425  <train> Loss:1.002  Acc:0.973\n",
      "220/425  <train> Loss:1.002  Acc:0.973\n",
      "230/425  <train> Loss:1.002  Acc:0.973\n",
      "240/425  <train> Loss:1.002  Acc:0.973\n",
      "250/425  <train> Loss:1.002  Acc:0.974\n",
      "260/425  <train> Loss:1.002  Acc:0.974\n",
      "270/425  <train> Loss:1.002  Acc:0.974\n",
      "280/425  <train> Loss:1.002  Acc:0.974\n",
      "290/425  <train> Loss:1.002  Acc:0.974\n",
      "300/425  <train> Loss:1.002  Acc:0.975\n",
      "310/425  <train> Loss:1.002  Acc:0.975\n",
      "320/425  <train> Loss:1.002  Acc:0.975\n",
      "330/425  <train> Loss:1.001  Acc:0.975\n",
      "340/425  <train> Loss:1.001  Acc:0.975\n",
      "350/425  <train> Loss:1.001  Acc:0.975\n",
      "360/425  <train> Loss:1.001  Acc:0.975\n",
      "370/425  <train> Loss:1.001  Acc:0.975\n",
      "380/425  <train> Loss:1.001  Acc:0.975\n",
      "390/425  <train> Loss:1.001  Acc:0.975\n",
      "400/425  <train> Loss:1.001  Acc:0.976\n",
      "410/425  <train> Loss:1.001  Acc:0.976\n",
      "420/425  <train> Loss:1.001  Acc:0.976\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'double'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_141121/1472377614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model = train(train_loader=train_dl, model=model, criterion=criterion, \n\u001b[0m\u001b[1;32m      7\u001b[0m               optimizer=optimizer, num_epochs=hps.num_epochs, ds_size=len(train_ds), device=device, batch_size=hps.batch_size)\n",
      "\u001b[0;32m/tmp/ipykernel_141121/3648635044.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, num_epochs, ds_size, device, batch_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mds_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mds_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"<{phase}> Loss:{epoch_loss:.3f}  Acc:{epoch_acc:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'double'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('srws': conda)"
  },
  "interpreter": {
   "hash": "7f2d3241de56b0fa9ccb32ec1dbd92e097a59df5b8abdff3a1f1414152af23a6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}